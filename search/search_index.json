{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>James is an entity that performs certain actions. See documentation.</p>"},{"location":"interpn/","title":"2023-11-24: Building a Hypercube Interpolator (in Rust)","text":"<p>Edited on 2023-11-30 to add more implementation details.</p>"},{"location":"interpn/#results","title":"Results","text":"<p>The TL;DR is that we can achieve a 10-150x speedup over the reference (FITPACK via scipy) when interpolating at a small number of observation points while eliminating allocation (!) and staying close to perf parity for larger inputs.</p> <p>This is particularly relevant to engineering optimization, where differentiation workloads for interpolators typically rely on forward differences that make many function calls with a small number of inputs.</p> <p>Similarly, in electrical, fluid, or thermal simulation, sim times are often bound to overhead from making many calls for a small number of outputs from real-material property data tables through an interface such as CoolProp, and this is compounded for optimization through such simulations.</p> <p>Finally, some perf charts for a reference case on fair ground: interpolation at (pre-selected) random locations inside a 20x20x20 3D grid.</p> <p>Without preallocation of outputs for the methods described here: </p> <p>Unlike the scipy bindings, the methods presented here can evaluate into preallocated output storage.</p> <p>With preallocation of outputs for the methods described here: </p> <p>Hold on - did they get worse with preallocation? No! This is an artifact of the benchmarking process, but one that accentuates the value of eliminating allocation. The benchmarks are interleaved (method 1 x100, method 2 x100, method 3 x100, ...), so they see some performance cross-talk through allocation.</p> <p>Eliminating allocation from the <code>interpn</code> methods significantly improves scipy's performance in neighboring benchmarks, despite both methods using an un-instrumented warmup and manually calling python's garbage collector between benchmarks. The scipy method is still allocating, it just has less overhead - so improving <code>interpn</code>'s memory performance by evaluating into a preallocated array helps scipy even more than it helps <code>interpn</code>. Quite an on-the-nose example of the value of avoiding allocation in application-level code!</p>"},{"location":"interpn/#rationale","title":"Rationale","text":"<p>First, a discussion of the state of the art, and why one might bother digging into a field that's surely been a solved problem for some time.</p>"},{"location":"interpn/#fitpack-exists","title":"FITPACK exists","text":"<p>As far as I'm aware, the last and only meaningful work done on general and reusable interpolation methods for scientific computing was Dierckx's excellent 1994 textbook and the accompanying Fortran library, FITPACK.</p> <p>The Dierckx FITPACK library has since been absorbed into netlib alongside BLAS and LAPACK, and remains under a homebrewed license.</p> <p>As such, it is usually integrated into other ecosystems as a user-side dependency, often in addition to all of the dependencies required to compile Fortran and produce bindings in the target language.</p> <p>Notably, the scipy project includes bindings to many FITPACK routines. In Rust, the splinify library provides bindings to select splining routines, although I have not been able to get through the game of dynamic dependency whackamole to get it to build successfully.</p> <p>FITPACK supports many pieces of functionality that are out of scope here -  in particular, Dierckx gives a thorough and performant treatment to fitting the interpolator coefficients, smoothing, and surface identification, often by finding fairly inspired phrasings of fitting problems as quadratic programs regardless of the polynomial order of the curvefits.</p>"},{"location":"interpn/#this-is-not-fitpack","title":"This is not FITPACK","text":"<p>The goal of this project is not to replicate all of FITPACK's functionality. In fact, curve fitting in its entirety is out of scope. Instead, this project will focus on the interpolation tasks common in scientific computing, which (possibly after some uncertainty-weighted fitting) typically hold a hard requirement on the interpolator reproducing the anchor values exactly.</p> <p>Unlike fitting and interpolation for graphics, the correctness of the result is the highest priority, followed by performance and ergonomics in close succession. Smoothing or other manipulation of the data are anti-goals, as they directly compromise correctness.</p>"},{"location":"interpn/#this-is-something-more-specific","title":"This is something more specific","text":"<p>Good quality scientific methods for curvefitting are a larger topic than interpolation, and while there are surely patterns in the process, those methods also tend to be fairly bespoke to the problem under examination.</p> <p>Functionality goals</p> <ul> <li>Evaluation of values (and later, gradient and hessian entries) for exact interpolators in Rust</li> <li>Self-contained (evaluate using only grid locations and data values)</li> <li>Avoid the need for another program or library to provide spline knots, etc</li> <li>Clear and permissive licensing</li> <li>Convenient, structured, and version-controlled build process</li> <li>Convenient inter-languge interop &amp; first-class support for bindings</li> <li>Compatibility with embedded and high-performance systems (no allocation)</li> <li>Smooth transition from interpolation to extrapolation (to support solvers)</li> <li>Separation of bounds error checking from interpolation/extrapolation (to support solvers)</li> </ul>"},{"location":"interpn/#tooling","title":"Tooling","text":"Rust Python Repo interpn interpnpy Docs Docs.rs readthedocs w/ mkdocs Benchmark Criterion timeit w/ warmup Lint rustfmt &amp; clippy ruff &amp; pyright Test cargo test pytest &amp; mktestdocs Release release-plz &amp; cargo-semver-checks maturin &amp; PyO3"},{"location":"interpn/#implementation-selecting-a-method","title":"Implementation: selecting a method","text":"<p>First, there are three approaches to interpolation on a regular or rectilinear grid that I've come across, and I had to choose one.</p> <ul> <li>Recursive (like FITPACK)</li> <li>Convolution (as used in GPU libraries)</li> <li>Geometric (no implementations I'm aware of)</li> </ul>"},{"location":"interpn/#recursive-method","title":"Recursive method?","text":"<p>The recursive method is fairly intuitive in addition to being fairly performant, and making no compromise on correctness or generality. The idea is to interpolate on one axis at a time, projecting the grid cell into lower dimensions until finally arriving at a point.</p> <p></p> <p>It's easy enough to imagine how this recursive method would generalize to extrapolation or to higher dimensions - do nothing, and it will handle those cases just fine.</p> <p>However, the amount of data that needs to be actualized at each step of the recursion varies with the number of dimensions - <code>O(2^(ndims - 1))</code> per observation point, plus or minus a factor of 2 depending on how hard one would like to optimize it.</p> <p>Whether or not this is a lot depends on circumstance - but it'd sure be neat if we could find a way to use a constant amount of storage, or at least an amount that scales more favorably with the number of dimensions.</p>"},{"location":"interpn/#convolutional-method","title":"Convolutional method?","text":"<p>The convolutional method doesn't make much of a diagram, but it's exceptionally fast on GPU for image scaling, and generalizes to higher polynomials. Unfortunately, it doesn't handle points near the edge of the grid very well, nor extrapolation - in fact, its performance gains are directly the product of sacrificing both correctness and generality.</p> <p>You also have to develop the footprint coefficients specifically for every dimensionality, and while I'm sure there's some series formulation of it similar to how finite difference coefficients are tabulated, I have no desire to develop that formulation.</p> <p>It's excellent for resampling images, and there's a huge market for that, but it's not quite what's needed for scientific computing.</p>"},{"location":"interpn/#geometric-method","title":"Geometric method","text":"<p>The geometric method, as far as I can tell, only exists as a blurb on wikipedia intended to give an intuitive explanation of the behavior of the better methods.</p> <p>The idea is that each grid cell can be partitioned into sub-cells characterized by the vector from the observation point to each grid cell vertex. This partitioning fully fills the grid cell. A multilinear interpolation can be obtained as the sum of the values at each grid cell multiplied by the area of the opposing sub-cell.</p> <p></p> <p>It's intended as an educational aid, but as an algorithm, it has some interesting properties that merit a harder look.</p> <p>First, the memory scaling is excellent: only <code>O(ndims)</code> stack usage is ever needed simultaneously per observation point.</p> <p>Second, it is better-positioned to avoid floating point error due to aggregating the contribution from each of the grid cell vertices simultaneously (a sum of products) rather than the recursive method's long chain of alternating subtractions, divisions, and multiplications. In fact, division is eliminated from intermediate steps entirely, and the longest chain of error-prone add or sub operations is of a constant length 2 instead of length <code>ndims</code>. The longest chain of multiplications is <code>ndims</code>, but only one division is required per observation point regardless of dimensionality, and multiplications are a relatively minor contributor to float error compared to addition and subtraction.</p> <p>Third, it may actually be more correct, in a sense, than the recursive method, because under extrapolation in corner regions, it will not extrapolate the derivatives on each axis the way that the recursive method does.</p>"},{"location":"interpn/#finding-the-grid-cell","title":"Finding the grid cell","text":"<p>Regardless of the choice of method, we'll always need to know what grid cell the observation point lands in, in order to know which tabulated values contribute to the interpolated result.</p>"},{"location":"interpn/#regular-grid","title":"Regular grid","text":"<p>For regular grids where all the grid locations are evenly spaced, this is logically easy if we're inside the grid:</p> <pre><code>// for some observation point coordinate `x`\nlet int_loc = ((x - grid_start) / grid_step) as usize;\n</code></pre> <p>It gets a little more complicated when we consider that the point may be outside the grid, in which case we need to snap to the grid:</p> <pre><code>let int_loc = (((x - grid_start) / grid_step) as usize)\n              .min(grid_size - 1)\n              .max(0);\n</code></pre> <p>But wait - we can't actually represent a value below zero as a usize, so we need a stop over as a signed integer to represent extrapolation below the grid without integer overflow:</p> <pre><code>let int_loc = (((x - grid_start) / grid_step) as isize)\n              .min(grid_size - 1)\n              .max(0) \n              as usize;\n</code></pre> <p>We can't acturally run that yet, though, because we can't just <code>as isize</code> and <code>as usize</code> a generic float type.</p> <p>Converting a float to an integer also isn't exactly a clear and concise operation - it can be done sketchily with a handful of bit shift mask operations, but to produce a correct result in general, and to avoid harmful edge cases, is not always possible, and we may have to generate an error here to avoid returning an incorrect result.</p> <p>Our observation point <code>x</code> is a generic <code>T: Float</code> using num_traits to generalize over <code>f32</code> and <code>f64</code>. In order to also capture safe typecasting, we can add another trait from num_traits, <code>NumCast</code>.</p> <pre><code>let floc = (x - grid_start) / grid_step); // Float location\nlet iloc = &lt;isize as NumCast&gt;::from(floc).unwrap(); // Signed integer location\nlet uloc = iloc.min(grid_size - 1).max(0) as usize;  // Usable unsigned index\n</code></pre> <p>That got a little weird, and there's a conspicuous <code>unwrap()</code> there: in fact, this is expected, because if we ever try to convert, say, a NaN value, or a value larger than <code>isize::MAX</code>, we'll have a real un-handleable error that needs to propagate to prevent the method from returning a genuinely incorrect result.</p>"},{"location":"interpn/#rectilinear-grid","title":"Rectilinear grid","text":"<p>For a rectilinear grid, we have an uneven spacing of grid cells on each axis. This means that in general, we're not sure exactly where a given observation point lands in the grid until we check it against the grid values.</p> <p>If we don't know anything about the location of the observation point other than that it's more likely to be inside the grid than outside, then a bisection search is the theoretical optimum for finding the grid index.</p> <p>The Rust core library implements an excellent binary search, which we can use via the <code>slice::partition_point()</code> interface.</p> <pre><code>let uloc = grid.partition_point(|v| *v &lt; x);\n</code></pre> <p>In practice, the simplicity gets engineered out of it to handle extrapolation in a similar way to what happened with the regular grid method, but that's the core of the idea.</p>"},{"location":"interpn/#bagging-the-memory-scaling","title":"Bagging the memory scaling","text":"<p>The improved memory scaling of the geometric method only holds if we can visit  the vertices of a given grid cell one by one, accumulating the contribution of each one before moving to the next. The value accumulated from each vertex doesn't depend on the others, so it works out in terms of the flow of information - but there are <code>2^ndims</code> vertices to visit.</p> <p>The usual solution for visiting all of them would be something like itertools' multi_cartesian_product - but this requires the standard library, and in order to actually access all of the indices at once, we'd still end up actualizing all <code>2^ndims</code> vertices at once, breaking the <code>O(ndims)</code> memory scaling.</p> <p>Starting with the index of the lower corner of the grid cell, we can represent each vertex in terms of an array like <code>[bool; ndims]</code> where each entry is the index offset from that lower corner to the current vertex.</p> <p>Ultimately, we need to get one row at a time out of a table like this:</p> dim 2 1 0 offset 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 <p>If we don't mind some integer operations, we can actually formulate this table one at a time by keeping a running record of the index offset, and flipping the index of each dimension one at a time every <code>2^dim</code> indices:</p> <pre><code>// In a loop to examine the vertices one at a time...\nlet mut ioffs = [bool; ndims];\nlet nverts = 2.powi(ndims);\nfor i in 0..nverts {\n    // Every 2^nth vertex, flip which side of the cube we are examining\n    // in the nth dimension.\n    //\n    // Because i % 2^n has double the period for each sequential n,\n    // and their phase is only aligned once every 2^n for the largest\n    // n in the set, this is guaranteed to produce a path that visits\n    // each vertex exactly once.\n    for j in 0..ndims {\n        let flip = i % 2_usize.pow(j as u32) == 0;\n        if flip {\n            ioffs[j] = !ioffs[j];\n        }\n    }\n\n    // ... then accumulate the contribution of the vertex\n}\n</code></pre> <p>So with that, we have no-std and limited-memory version of multi_cartesian_product specialized to this usage, and we only need to store a tiny number of values to run it.</p> <p>The cost comes in compute, but ultimately, integer mod with a base that is a power of 2 is not expensive. In any case, this compute has to happen somewhere, and it may as well be here.</p> <p>Doing this mod operation with manually-assembled bit-shift operations does not improve performance. I haven't looked too hard at the assembly, but I'd bet that's because the compiler knows what's up.</p>"},{"location":"interpn/#reinventing-the-wheel-indexing-into-an-array","title":"Reinventing the wheel (indexing into an array)","text":"<p>Ok, well, now that we have the vertex, we're still not quite ready to start accumulating the contribution. First, we need to get the grid value associated with that vertex.</p> <p>Without any external dependencies on array libraries, that means writing our own array indexing.</p> <p>Assuming we've got a contiguous input array that was formulated from C-style interleaved inputs from an <code>N x M</code> coordinate grid, the values are ordered like this:</p> <pre><code>[\n  z(x0, y0), ... , z(x0, yM),\n  z(x1, y0), ... , z(x1, yM), \n  ...\n  z(xN, y0), ... , z(xN, yM)\n]\n</code></pre> <p>In order to know where to find each row, column, or whatever you call the flakes of an array in the higher dimensions, we need to skip the cumulative product of the sizes of all the higher dimensions in order to find the next flake of the lower dimension.</p> <pre><code>// Sometime before the loop, accumulate the cumulative product\n// of sizes of each dimension of the grid\n\n// Populate cumulative product of higher dimensions for indexing.\n//\n// Each entry is the cumulative product of the size of dimensions\n// higher than this one, which is the stride between blocks\n// relating to a given index along each dimension.\nlet mut dimprod = [1_usise; ndims];\nlet mut acc = 1;\n(0..ndims).for_each(|i| {\n    dimprod[ndims - i - 1] = acc;\n    acc *= self.dims[ndims - i - 1];\n});  // This doesn't quite work as a .fold()\n\nfor i in 0..nverts {\n    //  ... sometime later in the loop where we did some shenanigans\n    //      with a mod operation to find the `ioffs` entries\n\n    // Accumulate the index into the value array,\n    // saturating to the bound if the resulting index would be outside.\n    let mut k = 0;\n    for j in 0..ndims {\n        k += dimprod[j]\n            * (origin[j] + ioffs[j] as usize).min(self.dims[j].saturating_sub(1));\n    }\n\n    // ... then, maybe, finally, accumulate the contribution of the vertex\n}\n</code></pre> <p>That expanded a bit from concept to execution, but it's also one of the places where Rust's particulars shine: the <code>saturating_{op}</code> functions are really excellent for doing safe indexing, and while they may represent extra operations in some places, usually wherever there's a <code>saturating_{op}</code>, there's a slice indexing bounds check that can be optimized out.</p> <p>And since we're examining extrapolation as well as interpolation, these indexing edge cases aren't academic - they're the product.</p>"},{"location":"interpn/#doing-interpolation-with-the-geometric-method-is-easy","title":"Doing interpolation with the geometric method is easy","text":"<pre><code>let mut interped = T::zero(); // The accumulated interpolated value\nfor i in 0..nverts {\n    //  ... sometime later in the loop where we did some shenanigans\n    //      with a mod operation to find the `ioffs` entries\n    //      and after we hand-roll array indexing\n\n    // Get the value at this vertex\n    let v = vals[k];\n\n    // Find the vector from the opposite vertex to the observation point\n    let mut dxs = [T::zero(); ndims];\n    for j in 0..ndims {\n        // Populate dxs[j] with the (unsigned) vector\n    }\n\n    // Accumulate contribution from this vertex\n    if !extrapolating {\n        // Interpolating is easy! Just do a cumulative product\n        // to get the sub-cell volume for this vertex which\n        // tells us its weight in the weighted average\n        let vol = dxs.iter().fold(T::one(), |acc, x| acc * *x);\n        interped = interped + v * vol;\n    }\n    else {\n      // Extrapolating\n      // ...\n    }\n}\n\n// Return the interpolated total, dividing through by the total\n// cell volume just one time at the end\ninterped / cell_vol\n</code></pre>"},{"location":"interpn/#extending-the-geometric-method-to-extrapolation","title":"Extending the geometric method to extrapolation","text":"<p>The geometric method sounds good on paper, but let's see how it holds up in a practical implementation. In particular, we need it to work well  (1) in extrapolation and (2) in higher dimensions.</p> <p>I filled up a notebook with doodles and algebra figuring this out, but I'll spare us both the excessive summations and keep the abuses of notation between me and the graph paper. Visual depictions of computational geometry fare better than obfuscatory equation soup, anyway.</p>"},{"location":"interpn/#extrapolation-without-adjustment","title":"Extrapolation without adjustment","text":"<p>Let's try extrapolating in 2D.</p> <p></p> <p>Well, that didn't work - at least not without some extra handling.</p>"},{"location":"interpn/#extrapolation-by-extending-the-grid","title":"Extrapolation by extending the grid?","text":"<p>One option would be to extend the grid cell to include the full area including the observation point. </p> <p>On inspection, this is the same as the recursive method - we would need to treat each dimension in extrapolation one at a time, actualize the new grid, then finish the interpolation (which, in this case, would just be taking one corner of the grid as the final value).</p> <p>That's not great. For one thing, it would have a worse memory scaling, and I'd be implementing both methods instead of just one. For another, it produces a dubious result when the grid data describes a nonlinear function: the extrapolation would twist the grid cell like a potato chip, effectively extrapolating the derivatives as well as the value and producing a kind of quadratic extrapolation instead of a linear one.</p> <p>There's a better way, but it involves chewing a bit of glass.</p>"},{"location":"interpn/#extrapolation-by-a-pile-of-extra-logic","title":"Extrapolation by a pile of extra logic","text":"<p>I don't have a good name for this process. It's a turducken of constructive geometry and plain algebra - basically reverse-engineering the failure of the method to naturally extend to extrapolation, and eliminating those failures.</p> <p>We clearly need to zero-out the contribution from vertex <code>A</code> as we move into extrapolation to produce the same result we would get by taking the finite difference from <code>B</code> to <code>C</code> and from <code>D</code> to <code>C</code>, and extrapolating linearly on each axis -  the hand-written extrapolation wouldn't include a contribution from from <code>A</code> at all.</p> <p>We also need to negate the contribution from the grid points on the interior of the grid - the areas that split the grid cell evenly before are now overlapping, and the result we need is no longer any weighted average of the grid values. But, imagining that we were in 3D, we could be still need to do some grid-cell-based weighted-averaging, because we could be extrapolating on two axes (as in the 2D example) while interpolating on a third axis.</p> <p>Once we've zeroed-out the contribution from the inner point <code>A</code> and negated the contributions from points <code>B</code> and <code>D</code>, we have another problem.</p> <p>All three non-zeroed points <code>B</code>, <code>C</code>, and <code>D</code> share an overlapping region which has an area that scales not linearly but quadratically with the position of the observation point. This can't be part of the final interpolated result, but it's easy enough to find the area of the nonlinear region and remove it. This removal process is, conveniently, still an O(1) process for each vertex of the grid cell, so while it makes extrapolation slightly slower, it doesn't change the overall perf scaling.</p> <p></p>"},{"location":"interpn/#extending-the-geometric-method-to-higher-dimensions","title":"Extending the geometric method to higher dimensions","text":"<p>Well, that's settled, right? It's not, but you get to find out the way I found out - after slogging through a 3D implementation.</p> <p>Let's see how it looks in 3D, interpolating to points on the interior of the grid.</p> <p>To start, we'll look at a point that's on one face of the grid cell, so that we can see what's going on without a forest of lines getting in the way, and leaving two of the sub-cells un-shaded to unclutter the diagram.</p> <p></p> <p>So far, so good. With a bit of staring and thinking, it's clear that this produces the same behavior as the 2D version as the observation point moves around the face of the cube.</p> <p>As it moves to the interior of the cube, things get a little more detailed.</p> <p></p> <p>Here I'm only shading two sub-cells again to keep the clutter down, but it's clear to see that the partitioning still fills the space, and the volume of each sub-cell scales linearly with the observation point's position on each axis.</p> <p>Anyway, it's all working well so far! Let's see how the fixes from the 2D method hold up under extrapolation in a side region.</p> <p></p> <p>Looks good! Only two of the regions are shaded again, but from this, we can already tell that negating the contribution from the points on the interior of the grid will have the intended effect, same as in 2D.</p> <p>Next, a look at the corner region, where we can exercise the fixes that zero-out the contribution from the entirely-inside vertex and trim the nonlinear region.</p>"},{"location":"interpn/#a-problem-in-higher-dimensions","title":"A problem in higher dimensions","text":"<p>Uh-oh. Only shading the nonlinear region(s) this time, because that's the catch: there isn't just one kind of nonlinear region, and the fix based on the 2D method only capture the cubic region, not the quadratic regions.</p>"},{"location":"interpn/#a-solution-in-higher-dimensions","title":"A solution in higher dimensions","text":"<p>It turns out, we can handle this, too - but not by enumerating all of the kinds of nonlinear region in higher dimensions. Or at least, I don't know how to do that. Instead, we can observe that all of the linear regions are of the same form, the rectangular prisms projecting from each face of the cube. We can also make a real claim that this will be the case in all higher dimensions, because any region that grows in a direction not aligned with the coordinate axes will have a nonlinear volume scaling, so all the linear regions must be aligned with the coordinate axes.</p> <p>So, rather than removing all the bad regions, which only works well in 2D, we can cycle through all the axes and take all the good regions that are projected from the n-cube's faces. This works, including in higher dimensions, and produces a true linear extrapolation outside the grid. However, it does cost in performance: for the corner regions where this culling process is required, the cost of evaluating an observation point scales like <code>O(2^ndim + ndim^2)</code> instead of just <code>O(2^ndim)</code>.</p> <p>For practical purposes, this is not too bad, but it is noticeable - about a factor of 2-4 reduction in throughput when extrapolating in corner regions, which also breaks timing guarantees (not ideal for, say, embedded control systems).</p> <p>With that, we have a method that generalizes to arbitrary dimensions, or at least to the 10 dimensions I've tested it in, under both interpolation and extrapolation in every type of region. And it can do this with no allocation (except, possibly, for somewhere to put the output) and with stack usage that scales linearly in the number of dimensions.</p>"},{"location":"interpn/#next-steps","title":"Next Steps","text":"<p>I am happy to report that due to the efforts of the cargo-semver-checks, release-plz, maturin, and PyO3 developers, I can gloss over the relatively painless process of releasing the rust library on crates.io, generating python bindings, and releasing the python bindings on PyPI.</p> <p>Instead of agonizing over tooling and processes, I can focus on extending the existing functionality. Next on the list, in any particular order that I find inspiration, are:</p> <ul> <li>Proper multi-cubic (Hermite spline) interpolation</li> <li>Vectorized alternatives to the multilinear methods here (for the high-throughput and timing-sensitive regimes)</li> <li>Triangular and tetrahedral mesh interpolation</li> <li>Perf optimizations of the python bindings for the existing multilinear interpolators</li> </ul>"},{"location":"strobe/","title":"2023-10-21: Array Expressions without Allocation (in Rust)","text":""},{"location":"strobe/#overview","title":"Overview","text":"<ul> <li>Github: https://github.com/jlogan03/strobe</li> <li>Docs: https://docs.rs/crate/strobe/latest</li> <li>Benchmarks: https://github.com/jlogan03/strobe/pull/6</li> <li>Tooling</li> <li>Checking assembly &amp; vectorization recipes: Compiler Explorer &amp; cargo-asm</li> <li>Benchmarking &amp; perf analysis: criterion &amp; valgrind</li> <li>Linting: rustfmt &amp; clippy</li> <li>Release &amp; versioning: release-plz &amp; cargo-semver-checks</li> </ul> <p>Strobe provides fast, low-memory, elementwise array expressions on the stack. It is compatible with no-std (and no-alloc) environments, but can allocate for outputs as a convenience. Its only required dependencies are <code>num-traits</code>, <code>libm</code>, and rust <code>core</code>.</p> <p>In essence, this allows a similar pattern to operating over chunks of input arrays, but extends this to expressions of arbitrary depth, and provides useful guarantees to the compiler while eliminating the need for allocation of intermediate results and enabling consistent vectorization.</p>"},{"location":"strobe/#development-process","title":"Development Process","text":"<p>This crate started as a thought-experiment using Rust's benchmarking utilities on nightly to explore how much of a typical array operation's runtime was related to allocation and how much was the actual numerical operations. As it turned out, under sporadic compute loads (meaning, with a cold heap) more than half of the run time of a vectorized array multiplication was spent just allocating storage.</p> <p>That observation won't be evident in the benchmarks below, because criterion does a number of cycles before the start of measurement in order to warm up the system and prevent allocation noise from being observed. This gives better quality data, but it's also important to keep in mind that under completely normal conditions, the pre-allocated and non-pre-allocated versions of these calculations give completely different performance results.</p> <p>I take this observation as evidence of a need to eliminate allocation from the evaluation of array expressions to the maximum possible extent - possibly leaving a single allocation at the end of the expression for the outputs.</p> <p>With this in mind, the bucket of parts that would become strobe moved to a single file in Compiler Explorer and experienced a number of iterations through making changes then doing ctrl-F for <code>mulpd</code> in the assembly to check whether the resulting calcs were vectorizing properly.</p> <p>The next key observation was that even with pre-allocated storage available on the heap, it was faster to copy segments of data into mutable storage on the stack than to send data back and forth from heap-allocated storage, even though this sometimes results in more copy events in the code as it is written.</p> <p>At this point, there's (1) a lot to gain by eliminating allocation for intermediate results and (2) not much to lose by copying data around in intermediate storage on the stack. This gives a natural form for the expression data structure and evaluation:</p> <ul> <li>Structure: Each expression node carries a fixed-size segment of storage just large   enough to take advantage of vector operations, with controlled memory alignment to make   sure we don't do any unaligned read/write.</li> <li>Evaluation: Each expression node populates its fixed storage by operating on data   from the fixed storage of its child nodes.</li> </ul> <p>Dead simple, no allocation, and no clever tricks to confuse the user. Each expression node does not need any information other than what its inputs are and what operation to perform - no multidirectional mutable linkages handled by a third orchestrator, no graph data structures, and exactly one lifetime bound in play.</p> <p>The evaluation takes place by pumping inputs and intermediate values from the leaves (inputs) through each operator node to the root:</p> <p>That's it. It's just a tree-structured expression, with no unnecessary fanciness. There's nothing in there that could be removed and leave it still working.</p>"},{"location":"strobe/#benchmarks","title":"Benchmarks","text":""},{"location":"strobe/#criterion","title":"Criterion","text":"<p>For large source arrays in nontrivial expressions, it is about 2-3x faster than the usual method for ergonomic array operations (allocating storage for each intermediate result).</p> <p>In fact, because it provides guarantees about the size and memory alignment of the chunks of data over which it operates, it is even faster than performing array operations with the full intermediate arrays pre-allocated for most nontrivial applications (at least 3 total array operations).</p> <p>This is demonstrated in a set of benchmarks run using Criterion, which makes every effort to warm up the system in order to reduce the overhead associated with heap allocations. Three categories of benchmarks are run, each evaluating the same mathematical operations over the same data:</p> <ol> <li>Slice-style vectorization, allocating for each intermediate result &amp; the output</li> <li>Slice-style vectorization, evaluating intermediate results into pre-allocated storage,    then allocating once for the output</li> <li>Strobe, using fixed-size intermediate storage on the stack, then allocating once for the output</li> </ol> <p>While the allocation for the output array might give an unnecessarily pessimistic view of the maximum throughput of the pre-allocated and Strobe versions of the calc, I believe this gives a comparison that is more relevant to common use cases.</p> <p>By the time we reach 4 multiplication operations in an expression, strobe sees about double the throughput of the other methods for array sizes larger than about 1e5 elements, and is only slightly slower than the fastest alternative for smaller sizes. The benefit for large array operations is large, and the penalty for small ones is small.</p> <p></p> <p>The worst case, where Strobe is used gratuitously for a single operation with a small array, is about 3x worse than just doing a contiguous slice operation:</p> <p></p> <p>The above results are obtained with a 2018 Macbook Pro (Intel x86). Similar scalings are obtained with a Ryzen 5 3600, with slight differences in the crossover point between methods, likely due to differences in cache size and memory performance. Results from both systems used for benchmarking and for more calculations are available here.</p>"},{"location":"strobe/#cachegrind","title":"Cachegrind","text":"<p>Unit tests use arrays of size 67, 3 more than the intermediate storage size of 64, in order to make sure that we visit the behavior of the system when it sees storage that is neither full nor empty in a given cycle. The first 64 values will be consistently vectorized and cached properly, as will the first two in the group of 3 values, so we can expect around 1/67th (1.5%) of the evaluations to miss the cache due to swapping from the vector loop to the scalar loop. Cachegrind run over the unit tests supports this napkin estimate:</p> <pre><code>jlogan@jlogan-MS-7C56:~/git/strobe$ valgrind --tool=cachegrind ./target/release/deps/strobe-5cb1bc954f8de3f1\n==24414== Cachegrind, a cache and branch-prediction profiler\n==24414== Copyright (C) 2002-2017, and GNU GPL'd, by Nicholas Nethercote et al.\n==24414== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info\n==24414== Command: ./target/release/deps/strobe-5cb1bc954f8de3f1\n==24414==\n--24414-- warning: L3 cache found, using its data for the LL simulation.\n\nrunning 33 tests\ntest test::test_atanh ... ok\n...(truncated)...\ntest test_std::test_std ... ok\n\ntest result: ok. 33 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.16s\n\n==24414==\n==24414== I   refs:      1,992,045\n==24414== I1  misses:       22,729\n==24414== LLi misses:        4,408\n==24414== I1  miss rate:      1.14%\n==24414== LLi miss rate:      0.22%\n==24414==\n==24414== D   refs:        736,224  (451,692 rd   + 284,532 wr)\n==24414== D1  misses:       15,937  (  8,971 rd   +   6,966 wr)\n==24414== LLd misses:        7,806  (  3,756 rd   +   4,050 wr)\n==24414== D1  miss rate:       2.2% (    2.0%     +     2.4%  )\n==24414== LLd miss rate:       1.1% (    0.8%     +     1.4%  )\n==24414==\n==24414== LL refs:          38,666  ( 31,700 rd   +   6,966 wr)\n==24414== LL misses:        12,214  (  8,164 rd   +   4,050 wr)\n==24414== LL miss rate:        0.4% (    0.3%     +     1.4%  )\n</code></pre>"}]}